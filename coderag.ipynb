{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from service import Service\n",
    "from utils import show_diff\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "\n",
    "model = \"gpt-4o\"\n",
    "dataset_name = \"humaneval\"\n",
    "main_folder = \"code_rag_bench\"\n",
    "eval_folder = f\"{main_folder}/{dataset_name}_{model}\".replace('-', '_')\n",
    "os.makedirs(eval_folder, exist_ok=True)\n",
    "failed_folder = f\"{eval_folder}/failed\"\n",
    "os.makedirs(failed_folder, exist_ok=True)\n",
    "\n",
    "s = Service(seed=42, get_usage=True, get_diff=True)\n",
    "\n",
    "dataset = load_dataset(f\"code-rag-bench/{dataset_name}\")[\"train\"]\n",
    "df = dataset.to_polars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_price(model, in_tk, out_tk):\n",
    "    prices_per_1k = {\n",
    "        \"gpt-4o\": {\"input\": 0.005, \"output\": 0.015},\n",
    "        \"gpt-4o-mini\": {\"input\": 0.00015, \"output\": 0.0006},\n",
    "    }\n",
    "\n",
    "    if model not in prices_per_1k:\n",
    "        raise ValueError(f\"Model {model} not found in the pricing list\")\n",
    "\n",
    "    input_cost = (in_tk / 1000) * prices_per_1k[model][\"input\"]\n",
    "    output_cost = (out_tk / 1000) * prices_per_1k[model][\"output\"]\n",
    "\n",
    "    return input_cost + output_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSES_FILE = f\"./{eval_folder}/benchmark.json\"\n",
    "\n",
    "r = {\"data\":[], \"checkpoint\": 0, \"input_tokens\": 0, \"output_tokens\": 0, \"total_price\": 0, \"price_per_req\": 0}\n",
    "if os.path.exists(RESPONSES_FILE):\n",
    "    with open(RESPONSES_FILE, \"r\") as f:\n",
    "        r = json.load(f)\n",
    "else:\n",
    "    with open(RESPONSES_FILE, \"w\") as f:\n",
    "        f.write(json.dumps(r, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 164/164 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "assert r['checkpoint'] == len(r['data']), \"Checkpoint does not match with executed requests\"\n",
    "print(f\"Done {r['checkpoint']}/{len(df)} ({100*r['checkpoint']/len(df):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in df[r['checkpoint']:].iter_rows(named=True):\n",
    "    print(f\"Task ID: {d['task_id']}\")\n",
    "    response = s.process(None, d['prompt'], model)\n",
    "    #show_diff(response['diff'])\n",
    "\n",
    "    try: idx_diff = max(i for i, item in enumerate(response['diff']) if item[0] == \"insert\")\n",
    "    except: idx_diff = -1\n",
    "    r[\"data\"].append({\n",
    "        \"solution\": response['updatedCode'],\n",
    "        \"only_solution\": response['diff'][idx_diff][-1] if idx_diff >=0 else response['updatedCode'],\n",
    "    })\n",
    "\n",
    "    r[\"input_tokens\"] += response[\"usage\"][\"input\"]\n",
    "    r[\"output_tokens\"] += response[\"usage\"][\"output\"]\n",
    "    r[\"checkpoint\"] += 1\n",
    "\n",
    "    if r['checkpoint'] % 10 == 0:\n",
    "        price = compute_price(model, r['input_tokens'], r['output_tokens'])\n",
    "        r['total_price'] = round(price, 4)\n",
    "        r['price_per_req'] = round(price / r['checkpoint'], 6)\n",
    "        with open(RESPONSES_FILE, \"w\") as f:\n",
    "            f.write(json.dumps(r, indent=4))\n",
    "        print(f\"SAVED ({r['checkpoint']})\")\n",
    "\n",
    "price = compute_price(model, r['input_tokens'], r['output_tokens'])\n",
    "r['total_price'] = round(price, 4)\n",
    "r['price_per_req'] = round(price / r['checkpoint'], 8)\n",
    "with open(RESPONSES_FILE, \"w\") as f:\n",
    "    f.write(json.dumps(r, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_tasks = []\n",
    "log = \"\"\n",
    "\n",
    "for row,_ in zip(df[:r['checkpoint']].iter_rows(named=True), r['data']):\n",
    "    row.update(_)\n",
    "\n",
    "    with open(f\"./{main_folder}/solution.py\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(row['solution'])\n",
    "    with open(f\"./{main_folder}/canonical_solution.py\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(row['prompt']+row['canonical_solution'])\n",
    "    with open(f\"./{main_folder}/evaluate.py\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(f\"from {main_folder}.canonical_solution import *\\n\")\n",
    "        f.write(row['test'])\n",
    "    \n",
    "    try:\n",
    "        exec(open(f\"./{main_folder}/canonical_solution.py\").read())\n",
    "        exec(open(f\"./{main_folder}/solution.py\").read())\n",
    "        exec(open(f\"./{main_folder}/evaluate.py\").read())\n",
    "        globals()['check'](globals()[row['entry_point']])\n",
    "    except Exception as e:\n",
    "        id = row['task_id']\n",
    "        realid = id.split('/')[1]\n",
    "        log += f\"\"\"Error ({id}): [{str(e.__class__).replace(\"<class '\", \"\").replace(\"'>\", \"\")}] {e}\\n\"\"\"\n",
    "        # renam solution and evaluate files for the failed task, so they can be debugged\n",
    "        if not os.path.exists(f\"./{failed_folder}/c{realid}_solution.py\"):\n",
    "            os.rename(f\"./{main_folder}/solution.py\", f\"./{failed_folder}/c{realid}_solution.py\")\n",
    "        if not os.path.exists(f\"./{failed_folder}/c{realid}_canonical_solution.py\"):\n",
    "            os.rename(f\"./{main_folder}/canonical_solution.py\", f\"./{failed_folder}/c{realid}_canonical_solution.py\")\n",
    "        if not os.path.exists(f\"./{failed_folder}/c{realid}_evaluate.py\"):\n",
    "            with open(f\"./{failed_folder}/c{realid}_evaluate.py\", \"w\", encoding='utf-8') as f:\n",
    "                f.write(f\"from c{realid}_canonical_solution import *\\n\")\n",
    "                f.write(f\"from c{realid}_solution import {row['entry_point']} as proposed\\n\")\n",
    "                f.write(row['test'])\n",
    "                f.write(f\"\\ncheck(proposed)\")\n",
    "        failed_tasks.append(id)\n",
    "\n",
    "success = r['checkpoint'] - len(failed_tasks)\n",
    "log += f\"Accuracy: {success}/{r['checkpoint']} ({success/r['checkpoint']:.2%})\\n\"\n",
    "log += f\"Failed tasks: {failed_tasks}\"\n",
    "\n",
    "with open(f\"{eval_folder}/benchmark.log\", \"w\") as f:\n",
    "    f.write(log)\n",
    "\n",
    "# remove the temporary files\n",
    "if os.path.exists(f\"./{main_folder}/solution.py\"):\n",
    "    os.remove(f\"./{main_folder}/solution.py\")\n",
    "if os.path.exists(f\"./{main_folder}/evaluate.py\"):\n",
    "    os.remove(f\"./{main_folder}/evaluate.py\")\n",
    "if os.path.exists(f\"./{main_folder}/canonical_solution.py\"):\n",
    "    os.remove(f\"./{main_folder}/canonical_solution.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[r['checkpoint'],'prompt'])\n",
    "print(response['updatedCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 'HumanEval/1',\n",
       " 'prompt': 'from typing import List\\n\\n\\ndef separate_paren_groups(paren_string: str) -> List[str]:\\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\\n    separate those group into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\\n    Ignore any spaces in the input string.\\n    >>> separate_paren_groups(\\'( ) (( )) (( )( ))\\')\\n    [\\'()\\', \\'(())\\', \\'(()())\\']\\n    \"\"\"\\n',\n",
       " 'canonical_solution': \"    result = []\\n    current_string = []\\n    current_depth = 0\\n\\n    for c in paren_string:\\n        if c == '(':\\n            current_depth += 1\\n            current_string.append(c)\\n        elif c == ')':\\n            current_depth -= 1\\n            current_string.append(c)\\n\\n            if current_depth == 0:\\n                result.append(''.join(current_string))\\n                current_string.clear()\\n\\n    return result\\n\",\n",
       " 'test': \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate('(()()) ((())) () ((())()())') == [\\n        '(()())', '((()))', '()', '((())()())'\\n    ]\\n    assert candidate('() (()) ((())) (((())))') == [\\n        '()', '(())', '((()))', '(((())))'\\n    ]\\n    assert candidate('(()(())((())))') == [\\n        '(()(())((())))'\\n    ]\\n    assert candidate('( ) (( )) (( )( ))') == ['()', '(())', '(()())']\\n\",\n",
       " 'entry_point': 'separate_paren_groups',\n",
       " 'docs': [{'text': 'from typing import List\\n\\n\\ndef separate_paren_groups(paren_string: str) -> List[str]:\\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\\n    separate those group into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\\n    Ignore any spaces in the input string.\\n    >>> separate_paren_groups(\\'( ) (( )) (( )( ))\\')\\n    [\\'()\\', \\'(())\\', \\'(()())\\']\\n    \"\"\"\\n\\n    result = []\\n    current_string = []\\n    current_depth = 0\\n\\n    for c in paren_string:\\n        if c == \\'(\\':\\n            current_depth += 1\\n            current_string.append(c)\\n        elif c == \\')\\':\\n            current_depth -= 1\\n            current_string.append(c)\\n\\n            if current_depth == 0:\\n                result.append(\\'\\'.join(current_string))\\n                current_string.clear()\\n\\n    return result\\n',\n",
       "   'title': 'separate_paren_groups'}],\n",
       " 'solution': 'from typing import List\\n\\ndef separate_paren_groups(paren_string: str) -> List[str]:\\n    \"\"\" Input to this function is a string containing multiple groups of nested parentheses. Your goal is to\\n    separate those group into separate strings and return the list of those.\\n    Separate groups are balanced (each open brace is properly closed) and not nested within each other\\n    Ignore any spaces in the input string.\\n    >>> separate_paren_groups(\\'( ) (( )) (( )( ))\\')\\n    [\\'()\\', \\'(())\\', \\'(()())\\']\\n    \"\"\"\\n    paren_string = paren_string.replace(\" \", \"\")\\n    result = []\\n    balance = 0\\n    current_group = []\\n\\n    for char in paren_string:\\n        current_group.append(char)\\n        if char == \\'(\\':\\n            balance += 1\\n        elif char == \\')\\':\\n            balance -= 1\\n\\n        if balance == 0 and current_group:\\n            result.append(\\'\\'.join(current_group))\\n            current_group = []\\n\\n    return result\\n',\n",
       " 'only_solution': '\\n    paren_string = paren_string.replace(\" \", \"\")\\n    result = []\\n    balance = 0\\n    current_group = []\\n\\n    for char in paren_string:\\n        current_group.append(char)\\n        if char == \\'(\\':\\n            balance += 1\\n        elif char == \\')\\':\\n            balance -= 1\\n\\n        if balance == 0 and current_group:\\n            result.append(\\'\\'.join(current_group))\\n            current_group = []\\n\\n    return result'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
